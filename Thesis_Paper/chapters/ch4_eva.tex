Given we have explained the detailed data synthesis algorithms in section \ref{chapter3:syn}, now we will focus on the data utility and risk assessment of synthetic data generated using parametric and non-parametric data synthesizers. In particular, we will discuss different data utility evaluation metrics suitable for different application scenarios. Besides that, approaches to measure the risk disclosure for both fully and partially synthetic data will be followed. By providing the evaluation metric in terms of utility and risk assessment, we aim to provide a basis for selecting an appropriate data synthesis algorithm for a specific use case, and for evaluating the trade-offs between utility and risk to ensure the quality on the synthetic dataset generated.

\subsection{Data Utility Evaluation}
\label{subsec:utility}
In the realm of evaluating the validity of data that has been altered to maintain confidentiality, a substantial amount of research has been conducted. A majority of these methods can also be utilized to assess the validity of synthetic data. In this chapter, we will concentrate on the measures that are particularly pertinent for synthetic data. For further examination, readers may refer to \citet{hundepool2012statistical}, which delves into additional evaluation measures.

Utility metrics, used to assess the effectiveness of data protection techniques, can be classified into three categories: global, outcome-specific, and fit-for-purpose. Global utility metrics assess the utility of the protected data by directly comparing it to the original data. These metrics have the advantage of not requiring any prior assumptions regarding the intended use of the synthetic data, however, as the utility is measured on a broad level, it does not necessarily ensure high utility for a specific analysis. In contrast, outcome-specific utility metrics measure the utility of the synthetic data for a specific analysis, such as the results of a linear regression model. Fit-for-purpose measures, such as graphical comparisons of the marginal and bivariate distributions of all variables, or consistency checks to prevent implausible values, form the initial stage of any utility assessment.
\subsubsection{Global Utility Metrics}
\label{subsubsec:global}
Utility assessment of synthetic data can be performed by comparing it directly to the original data. A common approach for this comparison is the utilization of distance measures, such as the Kulback-Leibler divergence \citep{karr2006framework} or Hellinger distance \citep{gomatam2003distortion}. However, computing these measures for large datasets can be challenging. An alternative approach, inspired by the literature on propensity score matching \citep{rosenbaum1983central}, aims to evaluate the distinguishability between the original and synthetic data. Propensity scores are estimated by stacking the $n_{org}$ original records and the $n_{syn}$ synthetic records and adding an indicator variable, with a value of 1 indicating the record is from the synthetic data and $0$ otherwise. Then, a model is fitted using the stacked data to estimate the propensity scores, which represent the probability of each record belonging to the synthetic data. If the synthetic data was an exact copy of the original data, it would not offer any information to distinguish between the two datasets, and the distribution of the estimated propensity scores would be the same. One way to measure the utility of the synthetic data is to evaluate the difference in the distribution of the propensity scores between the original and synthetic data. There are various metrics that can be used for this purpose, including the Kolmogorov-Smirnov distance, which is suggested by \citet{mckay2018differentially} and referred to as SPECKS (Synthetic data generation; Propensity score matching; Empirical Comparison via the Kolmogorov-Smirnov distance), or the Mannâ€“Whitney U test (Wilcoxon rank-sum test).

In recent years, the propensity score mean squared error ($p_{MSE}$) has emerged as a widely used metric for evaluating the validity of synthetic data \citep{woo2009global,snoke2018general}. The $p_{MSE}$ is calculated by considering the predicted values obtained from a model, fit on the stacked dataset containing both original and synthetic data, with the variable $c$ equal to $n_{syn}/N$, where $N = n_{org} + n_{syn}$ is the total number of records, which can be formatted as $\frac{1}{N} \sum_{i=1}^{N}(p_i-c)^2$. A lower $p_{MSE}$ value indicates a higher analytical validity of the synthetic data, as $p_i$ approaches $c$ when the model is unable to differentiate between the original and synthetic data, where $p_{i}$ stands for the predicted values ($i=1,...,N$). However, as noted by \citet{woo2009global}, the $p_{MSE}$ may increase with the number of predictors used in the model. To address this issue, \citet{snoke2018general} derived the expected value and standard deviation of the $p_{MSE}$ under the assumption that the synthesis model is correctly specified, and introduced two additional utility measures. The first measure is the $p_{MSE}$ ratio, which is the ratio of the empirical $p_{MSE}$ to its expected value under the null hypothesis. The second measure is the standardized $p_{MSE}$, a.k.a $Sp_{MSE}$, which is calculated as the difference between the empirical $p_{MSE}$ and its expected value under the null, divided by its standard deviation under the null.



\subsubsection{Outcome-specific Utility Measures}
\label{subsubsec:outcome}
The focus of these measures is explicitly on evaluating the suitability of the synthetic data for a designated analysis task. One simple way to assess the analytical validity is to graph the estimates obtained from the original data, such as means or regression coefficients, against the corresponding estimates obtained from the synthetic data. If the utility of the synthetic data is high, these coefficients should display a clustering pattern around the $45-$degree line.

The evaluation method discussed previously does not consider the inherent uncertainty of the estimates, which can be significant in certain situations. For instance, larger deviations between the estimates might be acceptable when the sampling error is high, such as when the estimate of interest is based on a small subset of the data. Conversely, the same deviation might not be acceptable if the statistic is based on the entire sample. To account for the uncertainty of the estimates, a widely adopted measure is the confidence interval overlap measure proposed by \citet{karr2006framework}. This measure assesses the average relative overlap between the confidence interval derived from the original data and the confidence interval derived from the synthetic data. A high overlap measure, close to $1$, indicates that the same inferential conclusions can be drawn, regardless of whether the analysis is based on the synthetic data or the original data.

The recent surge in the use of machine learning approaches has also led to the increased popularity of the machine learning efficacy metric, particularly in computer science literature. This type of utility measure, commonly referred to as model comparability measure, assesses the similarity in results obtained from machine learning models trained on synthetic data compared to models trained on original data. The evaluation procedure typically involves training the models of interest on both synthetic and original data, and then comparing the performance of these models based on the same set of test records obtained from the original data. If the commonly used evaluation criteria such as accuracy, $F1-$score, etc., are similar regardless of whether the models were trained on synthetic or original data, the utility of the synthetic data is considered high. Additionally, utility is sometimes evaluated by determining whether the use of synthetic data for model training leads to the same ranking of various machine learning models. For instance, if the original data suggests that a multi-layer perceptron (MLP) classifier outperforms a random forest, which in turn outperforms logistic regression, the same ranking should be obtained when the synthetic data is used for model training.
\subsubsection{Fit-for-purpose Measures}
\label{subsubsec:purpose}
The fit-for-purpose measures represent a preliminary evaluation when determining the utility of the generated data. They are distinguished from the other two measures as they do not necessarily concentrate on evaluating the validity of analyses deemed important for the data users. Nor do they aim to directly assess the similarity of the original and synthetic data through a single global statistic. The primary objective of these measures is to provide a preliminary evaluation of the quality of the synthetic data and to identify any areas of the synthesis process that may require further improvement. This category of measures can be classified into three categories: graphical evaluations, feasibility assessments, and calculation of various goodness-of-fit (GOF) measures.

Graphical evaluations in the assessment of the usefulness of generated data constitute a key strategy and include tactics such as the presentation of side-by-side comparisons of the marginal distributions of both the synthetic and original data, as well as contour plots for the examination of bivariate distributions. Additionally, visual examinations of conditional distributions, such as the income distribution across different demographic categories, can also be performed.

Plausibility checks, on the other hand, necessitate the involvement of subject-matter experts who are familiar with the data, as not all discrepancies can be readily discerned. For instance, while identifying anomalies such as the existence of two-year-old married individuals may be straightforward, determining the credibility of changes in annual turnover for a particular establishment within a specific industry may prove more challenging.

Finally, any GOF measure can be utilized to gauge the similarity between specific aspects of the synthetic and original data. For example, the Kolmogorov-Smirnov test statistic can be employed to assess the similarity of continuous variables in the dataset, while cross-tabulations of several variables can be evaluated through the utilization of the $\chi^2$ statistic or the likelihood ratio statistic. While the test statistic should not be used to test for statistically significant differences between the synthetic and original data, as the two samples cannot be treated as independent due to the synthetic data being generated based on information from the original data, the value of the test statistic can still be utilized to evaluate the efficacy of different synthesis techniques and to identify potential issues with the quality of the synthetic data. For instance, a high test statistic for many cross-tabulations involving the age variable may indicate that further improvement is required in the synthesis of this variable.

The $p_{MSE}$ measure described in Section \ref{subsubsec:global} can serve as a fit-for-purpose measure by only considering the relevant variables when computing the propensity score. \citet{raab2021assessing} provides an example of how this approach can be utilized to visualize the utility for bivariate distributions. Additionally, the R package "synthpop" implements graphical visualization tools for this purpose.

In the study conducted by \citet{raab2021assessing}, the authors investigated the correlation between various goodness-of-fit measures and found a high correlation ($>0.9$) among most of them. Notably, a correlation above $0.99$ was found between the adjusted $\chi^2$ test proposed by \citet{voas2001evaluating}, the Freeman-Tukey statistic, the Jensen-Shannon divergence (JSD), and the $p_{MSE}$, as well as between the Kolmogorov-Smirnoff test statistic, the Mann-Whitney test statistic, and two additional measures that were not further discussed for brevity. This result suggests that it is sufficient to utilize only one or two GOF criteria when evaluating the utility of the generated data.


\subsection{Risk Disclosure Analysis}
\label{subsec:risk}
The distinction between partial and full synthesis of data poses different risks from a disclosure protection perspective. With partial synthesis, there still exists a one-to-one correspondence between the original and the synthetic data. On the other hand, in full synthesis, such correspondence no longer holds, as the synthetic data do not necessarily have to be of the same size as the original data. This makes it difficult to measure the risk of re-identification, as commonly done for other disclosure protection methods \citep{reiter2005estimating, skinner2008assessing, shlomo2014probabilistic}. However, it does not imply that fully synthetic data cannot pose any risk of leaking sensitive information.

\citet{manrique2018bayesian} demonstrate using real data that when a fully conditional specification approach (a method commonly applied in multiple imputation for nonresponse) is utilized for synthesis based on CART, there is a risk that the synthesizer will replicate most of the original records. This occurs because the approach always conditions on all other variables in the dataset, leading to situations where the synthesized variable values are completely determined by the other variables in complex datasets containing many categorical variables. The CART synthesizer may become trapped in this situation, resulting in replication of records from the original data. Such a problem can be avoided by not using the fully conditional specification approach, which offers no advantages in the context of synthetic data.

However, this example highlights that fully synthetic data cannot be assumed to be free from the risk of disclosure of sensitive information. While measuring these risks is challenging, research in this area remains limited.

In this section, we initiate the process of reviewing the literature on the methods proposed to evaluate the risks associated with fully synthetic data. These evaluations can also be applied to partially synthetic data, while the risk assessments reviewed in the subsequent part of this section are only applicable to partial synthesis and aim to assess the risk of re-identification for the generated data.

\subsubsection{Risk Assessment for Fully Synthetic Data}
\label{subsubsec:riskfullsyn}
This section examines the methods that have been put forth in the literature for evaluating the risks associated with fully synthetic data. Although the connection between the original and synthetic data is severed in the case of full synthesis, some organizations still assess how many synthetic records have a one-to-one correspondence with the original data. This evaluation is driven by the perceived risk that survey respondents may be apprehensive if they find a synthetic record that precisely corresponds with their own record, particularly if their combination of attributes makes them unique in the original data.

Additionally, authors such as \citet{park2018data} and \citet{zhao2021ctab} have computed the distance between the synthetic data records and their nearest neighbors in the original data. The average of these distances across all synthetic records is then employed as a measure of risk. From a practical standpoint, the meaning of this risk measure is not clear. Even if the average distance is small, the distance for some records may be substantial. An attacker would not be able to determine which records have small distances, and a small distance does not necessarily indicate a risk if the closest record is in a densely populated area of the data distribution.

Another risk evaluation measure, proposed by \citet{taub2018differential}, matches cases from the original and synthetic data. The authors divide the variables in the dataset into key variables, which are assumed to be known by the attacker, and target variables, which the attacker is attempting to infer. They assume that the attacker focuses on records with low $l-$diversity for the target variables within a given equivalence class, defined by the key variables. Let $K$ represent the vector containing the key variables and $T$ represent the vector of target variables. The authors define the Within Equivalence Class Attribution Probability (WEAP) as follows:
\begin{align*}
    \label{equ:weap}
    WEAP_{j}=Pr(T_{j}|K_j)=\frac{\sum_{i=1}^{n}I(T_i=T_j,K_i=K_j)}{\sum_{i=1}^{n}I(K_i=K_j)},
\end{align*}
in which, the indicator function, I(Â·), equals to 1 when the statement inside the parenthesis is true and equals to 0 when it is false. The value of n refers to the size of the database. The authors in \citet{taub2018differential} concentrate on the synthetic records for which the value of $WEAP_j$ is equal to 1, and for those records, they calculate the Targeted Correct Attribution Probability (TCAP) with the following equation:
\begin{align*}
    \label{equ:tcap}
    TCAP_{sj}=Pr(T_{sj}|K_{sj})_{o}=\frac{\sum_{i=1}^{n}I(T_{o,i}=T_{s,j},K_{o,i}=K_{s,j})}{\sum_{i=1}^{n}I(K_{o,i}=K_{s,j})},
\end{align*}
with the subscript $s$ representing synthetic data and $o$ representing the original data. Note that the Targeted Correct Attribution Probability (TCAP) score is defined in a range between zero and one, with higher values denoting greater risk.

Another category of risk assessments for fully synthetic data concentrates on the fact that the synthesis models themselves may inadvertently expose information regarding the contents of the original data. For instance, the use of a fully saturated log-linear model in conjunction with vague prior information to synthesize a set of categorical variables may reveal the presence of specific attribute combinations in the synthetic data, implying that the same combinations must have been present in the original data. These types of risk assessments, referred to as membership attacks in the computer science literature, occur when an attacker gains knowledge that a particular record was included in the original data. The literature presents various methods for estimating the risks associated with membership attacks, which typically presume that the attacker has prior knowledge of the true values for some target records, using this information to determine whether these units are present in the original data \citep{stadler2022synthetic}. These assessments are premised on the assumption that the attacker is not interested in acquiring new information about a unit contained within the data, but only seeks to determine whether the unit was part of the original data. In some cases, learning this information may be deemed unacceptable, as certain laws specify that such risks must be avoided, and knowledge of inclusion in a database may also expose sensitive information, particularly if the database only contains a specific subset of the population, as seen in the example of the Survey of Prison Inmates conducted by the Bureau of Justice Statistics in the United States.

Despite the challenges involved, there exist risk measures for fully synthetic data that do not make the assumption that the attacker has knowledge of the original data. In the realm of inferential attacks, \citet{reiter2014bayesian} proposes a method for computing the posterior distribution $f(Y_i |D,X,M,d_{org}^{-i})$, where $Y_i$ represents the original value of a variable $Y$ for a unit $i$, $D$ represents the synthetic data, $X$ contains unchanged values from the original data (if full synthesis is employed, $X$ is empty), $M$ contains information regarding the synthesis model, and $d_{org}^{-i})$ denotes the original data excluding the record $i$. This method evaluates how much an attacker can learn about an unknown value $Y_i$ based on the synthetic data. If the posterior distribution of $Y_i$ exhibits low variability in comparison to the prior distribution prior to observing the synthetic data, the risk of disclosure is high. While the strong assumption that the attacker knows all information from the original data excluding record $i$ is not strictly necessary, it is often required for the purpose of making the problem computationally feasible. Nevertheless, even with these assumptions, this risk assessment method is only applicable when the number of variables in the data is very limited, as demonstrated in \cite{hu2014disclosure}.

In conclusion, assessing disclosure risks for fully synthetic data remains a complex challenge, and while the majority of researchers acknowledge that these data are not immune to risks, additional research is required to quantify these risks under realistic conditions.



\subsubsection{Risk Assessment for Partially Synthetic Data}
\label{subsubsec:riskpartialsyn}
In the context of partial synthesis, the risk measures discussed previously can also be applied. However, the existence of a unique match between the synthetic and original records implies that the risk assessment should focus on the potential for an attacker to re-identify records in the synthetic data. Building upon the work of \citet{reiter2005estimating}, \citet{reiter2009estimating} proposed methods to evaluate the re-identification risk in partially synthetic data.

Building on the work of \citet{drechsler2010sampling}, the process of evaluating the risk of re-identification for partially synthetic data can be summarized as follows. Given the information an intruder possesses on a specific target unit in the population, denoted by $t$, where $t_0$ represents the target's unique identifier, and $P_{i0}$ represents the unique identifier for the $i$th record in the synthetic data ($d_{syn}$), the intruder aims to determine the match between the target and the $i-$th record in $d_{syn}$ such that $P_{i0}= t_0$. The variable $J$ is a random variable that takes the value of $i$ when $P_{i0} = t_0$ for $i$ in $d_{syn}$. The intruder seeks to compute the probability $Pr(J = i|t, d_{syn}, S)$ for $i = 1, . . . , n$, where $S$ represents any information released about the synthesis models.

Since the intruder is not aware of the actual values of the synthesized variable ($Y^{âˆ—}$), the computation of the match probabilities must involve integrating over all possible values of $Y^{âˆ—}$. The following formula is then calculated for each record:
\begin{align}
    \label{equ:risk-partial}
    Pr(J = i|t, d_{syn}, S)=\int Pr(J = i|t, d_{syn}, Y^*,S)Pr(Y^*|t, d_{syn}, S)dY^*.
\end{align}
The risk computations shown in equation \eqref{equ:risk-partial} for partially synthetic data can be estimated through a Monte Carlo approach as proposed by \citet{drechsler2010sampling}. The intruder seeks to determine the probability, $Pr(J = i|t, d_{syn}, S)$, of matching unit $i$ in the synthetic data, $d_{syn}$, to a particular target unit in the population, $P$, based on information $t$ and the released information $S$ about the synthesis models. To estimate this probability, the intruder first samples a value of the synthesized variable, $Y^*$, from the probability distribution, $Pr(Y^âˆ—|t,d_{syn},S)$. This value is used in conjunction with the matching strategy, such as nearest neighbor matching, to compute the match probability. This two-step process is repeated a large number of times and the expected match probability is estimated as the average of these computed probabilities.

The disclosure risk can be measured using three risk measures proposed by \citet{reiter2009estimating}: the expected match risk, the true match rate, and the false match rate. The expected match risk is the sum of the number of true matches divided by the number of records with the highest match probability for the target. The true match rate is the percentage of true unique matches among the target records. The false match rate is the percentage of false matches among unique matches.

It should be noted that these risk assessments assume that the intruder knows that the target record is included in the released data. \citet{drechsler2008accounting} present extensions of this approach that also account for the additional uncertainty from sampling when the intruder does not know whether the target participated in the survey.




% \subsection{Analysis-specific Utility from Regression Models}
% \label{subsec:fit}
% ...

% \subsubsection{Analysis from Fitted Linear Regression Models}
% \label{subsubsec:fit-lm}

% \subsubsection{Analysis from Fitted Generalized Linear Regression Models}
% \label{subsubsec:fit-glm}

