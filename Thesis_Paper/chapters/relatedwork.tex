In this chapter, general definition of data privacy and the necessity to maintain data privacy are described. 
Furthermore, in order to keep data privacy, this chapter also presents an overview of methods to achieve the 
goal of privacy preserving data analysis and publication, which have been adopted in several domains. 
Here, we want to emphasize the use of synthetic data and one of its most popular applications, 
i.e., differentially private synthetic data, that are able to prevent disclosure in the process of synthetic data generation.

\subsection{Data Privacy}
\label{subsec:dataprivacy}
It is well-acknowledged that we have entered a data-driven world and data are 
often regarded as significant constituents for our society. At the same time, 
an open society can also learn from these data so as to develop feasible 
and practical policy guidelines \citep{evans2021statistically}. Especially 
during the outbreak of coronavirus disease 2019 (COVID-19), more and more increased concerns are raised that it is 
essential for a society to utilize such data, which are widely-spread in the population
and analyzed with regard to various perspectives, to advance sophiscated planning
and develop more concrete social welfare benefits for the citizens. Consequently,
both seen from the public health perspective and the economy perspective, the on-going
COVID-19 global pandemic serves as a rigid reminder that detailed data are urgently 
needed to assist in decision making, damage control scenarios. Regardless of prevalent
consensus reached to leverage more microdata, the inappropriate use of such 
information can cause harm in data confidentiality and privacy as sometimes the attacks
from an intruder may result in the leakage of an individual's sensitive information, e.g., identity, 
address and salary, etc.


On the premise of possible outcomes brought by the misuse of microdata, it is crucial that 
we encourage proper and legal use of the collected datasets. Holding this motivation, researchers 
have developed a variety of strategies aiming to avoid the disclosure of sensitive records while
revealing these specific information to the public \citep{duncan2011statistical}. In the early times, 
several traditional methods have been proposed to limit data disclosure with strategies like top-coding,
swapping or data suppression. Nevertheless, with increased computing power and more data access demanded by
the public, the risks of data disclosure are often seen as underestimated using simply these traditional
protection strategies, where instances of privacy breaches can be found simultaneously in the public and 
private sectors \citep{de2015unique}.

Alternatively, with the purpose to reach the trade-off between data disclosure protection and broad
data access, the idea of synthetic data has been released. When using this approach, we make a model
fitted to the microdata and the corresponding outputs from the fitted model are then used to replace
the original values in the previous information.


\subsection{An Overview of Data Synthesis Approaches}
\label{subsec:datasynthesis}
The field of employing synthetic data to avoid data statistical disclosure has been introduced by \citet{rubin1993statistical}
and \citet{little1993statistical} in the context of learning multiple imputation (MI) for nonresponse \citep{little2019statistical}.
In their work, they showed the possibility to get these sentitive values replaced with "imputed" values
rather than impute data for those missing records in the original dataset. With the application of this multiple
imputation framework, ramdom draws sampled from these imputed populations can then be circulated to the public. In more
extreme cases, when it is necessary to avoid the release of original data completely, those instances from
the original sample can also be displaced by samples from the imputation model.

Depending on the level of protection prone to specific scenarios, data synthesis methods are categorized to two groups, i.e.,
partial synthesis and full synthesis. For partially synthetic data, only some parts of the original records are synthesized. 
On the contrary, the entire dataset are replaced by synthetic values with the utilization of full synthesis methods.
It is evident to infer that the desired protection level is high when applying fully synthetic data methods, as original instances
are completely excluded. 



\subsubsection{Computer Science Approaches}
\label{subsubsec:csapproach}
Despite the origin and early development of synthetic data, in computer science, the data Synthesis
approach did not raise much interest in the study of data privacy until the advent of various privacy
standards. In order to cater to people's requirement of privacy protection, scientists have defined
several popular data privacy standards such as $k-$anonymity \citep{sweeney2002k}, $l-$diversity \citep{machanavajjhala2007diversity}
and $t-$closeness \citep{li2006t}. We try to shortly introduce the key ideas of the three popular
standards in the following context. 

\paragraph{$k-$Anonymity, $l-$Diversity and $t-$Closeness}
In 1998, \citet{sweeney2002k} has proposed the term of $k-$anonymity. When the information for every individual included in the data
cannot be discriminated from at least $k-1$ other individuals, the release of the database is considered to fulfill the
$k-$anonymity requirement. The works of \citet{machanavajjhala2007diversity}, \citet{truta2006privacy} and \citet{xiao2006personalized}
have exhibited that $k-$anonymity can successfully prevent identity disclosure and in this case it is impossible to identify
his exact data in the database. However, it does not guarantee attribute disclosure which indicates the revealing of a person's
sensitive attribute information even if we cannot access the exact record of instance from him/her. 

In order to overcome the limitations brought by $k-$anonymity, \citet{machanavajjhala2007diversity} introduced the definition of $l-$diversity in 2006 and the
essential idea lies in the prevention of homogeneity which often occurs in the equivalence classes with sensitive attributes.
By noting equivalence classes, we aim to refer those data instances that are unidentifiable from one another with masked information
in those non-sensitive attributes and thus can be categorized as an equivalence class. A database is considered to be $l-$diverse
if there are at least $l$ different values belonging to those sensitive attributes in each equivalence class. The shortcoming of $l-$ diverisity
is that this standard can be unnecessary to achieve from time to time. For instance, if we now have 10,000 rows of data with a sensitive attribute,
the distribution of the sensitive attribute is imbalanced where the attribute is 1 for $1\%$ of the individuals and 0 for the remaining
$99\%$. In an equivalence class with simply $0-$entries for the sensitive attribute, the individual would actually not mind being
recorded as negative for the sensitive attribute. But so as to achieve $2-$diversity, the number of equivalence classes would
then be $10,0000 \times 0.01 = 100$, which would cause a large amount of information loss.

The shortcomings of $l-$diverisity are addressed when \citet{xiao2006personalized} proposed the definition of $t-$closeness. We say that a database
is $t-$close when the distribution of the sensitive attributes in each equivalence class distinguishes from the distribution of the sensitive attributes 
in the complete table by a given threshold $t$ at most. The distance function used here is the Earth Mover Distance \citep{rubner2000earth} and it
calculates the minimal amount of endeavor required to transform one distribution to another by moving the mass distribution between those two.


However, the definition of synthetic data does not really fit into the requirement of the above examplified privacy standards. For example, a fully
synthetic dataset can still offer a high level of privacy protection but breaches the requirement of $k-$anonymity for any $k>1$.

\paragraph{Differential Privacy based Approaches}
The idea of Differential Privacy (DP) is introduced by \citet{dwork2008differential} in 2008, which has facilitated the application of
synthetic data in the computer science literature. In short, a database is said to be differential private when changing one
record in the database has a limited effect on the outputs of the running data mechanism. With a focus swittched to the data mechanism, 
the concept of DP situates better with the key ideas of data synthesis. In this case, finding a suitable synthesis mechanism which suffices
the requirements of Differential Privacy is the most essential part of the work. 


Based on the establishment of DP concept, \citet{barak2007privacy} has developed one of the first approaches leveraging a Fourier Transformation
and linear programming for those low-order contingency tables \citep{dwork2008differential}. There are also some other early applications, e.g., \citep{eno2008generating},
\citep{cano2010evaluation}, \citep{blum2013learning}, \citep{xiao2010differential}, etc. Besides, others have also established approaches adapting
ideas from the statistical community to the requirement of Differential Privacy. These works include \citep{abowd2008protective}, 
\citep{machanavajjhala2008privacy}, \citep{charest2011can} and \citep{mcclure2012differential}. Simultaneously, with the use of Bayesian networks, 
\citep{zhang2017privbayes} has accommodated the idea of synthetic data to synthesize multi-dimensional datasets, which is called PrivBayes. Moreover,
taking into account the dependency structure of the original data, Copula function has been utilized by \citet{li2014differentially} to combine the ideas from both techniques (DPCopula).

\paragraph{Generative Adversarial Networks based Approaches}
In parallel, proposed by \citet{goodfellow2020generative}, the idea of Generative Adversarial Networks (GANs) has caused an huge inspiration
in data synthesis research and practical applications in the corresponding computer science literature. Fundamentally, a GAN comprises two deep 
neural networks that are called a generator and a discriminator, respectively. The generator network basically generates fake data in order to
mislead the discriminator, whose task concerns the distinguishment of real data and fake data. In an iterative process, both networks
are improved. In this case, those final data generated by the generator network can be regarded as a variant of the synthetic data.
Early applications of GANs have focused on synthesizing image data, for example \citep{denton2015deep}, as GANs turned out to be extremely
successfully image detection, speech recognition and natural language processing. Nevertheless, researchers gradually turned their attention to
microdata synthesis (in the computer science literature, microdata can be understood as tabular data specifically in contrast to image data or other types of synthesis). 
But to adopt GANs for microdata synthsis requires additional concerns. Unlike those neighboring pixel data, microdata often consists of categorical variables
which can be sparse and correlations among them are often weaker. As a consequence, the positioning of variables with microdata is also
sparsely informative because of the independent structure of indivial instance records. Thus, the relationships between variables need to be 
additionally modelled without the help of spatial information. To address this issue and model relationships between variables, 
\citet*{xu2019modeling} developed the conditional tabular GAN (CTGAN) to overcome challenges from imbalanced categorical and multi-modal continuous data.
Furthermore, Causal Tabular GAN (Causal-TGAN) \citep{wen2021causal} shows the possibility to model for casual relationships between variables.

\paragraph{Other Techniques based Approaches}
Apart from the adoption of GANs, other data synthesis techniques relied on Variational Autoencoders \citep{vardhan2020generating,ma2020vaem},
copulas \citep{patki2016synthetic,kamthe2021copula}, or other strategies especially preserving certain marginal distributions \citep{mckenna2019graphical} have also
been developed recently. Here, we want to shortly cover the adaptation of Variational Autoencoders (VAEs) for synthetic data. Compared to GANs,
a typical VAE includes three networks: an encoder, a decoder and a discriminator, which are responsible for learning complimentary tasks. To be more specific,
the encoder maps the input data to a latent representation while the decoder tries to reconstruct and restore the data. For the discriminator, it decides
if the given sample is real data or data produced by the decoder network. The reconstruction error needs to be minimized during the training of VAE. 
One of its first applications is called the Triplet based Variational Autoencoder (TVAE), Proposed by \citet{xu2019modeling}, TVAE is adapted from VAE to tabular data, and
is trained using evidence lower-bound (ELBO) loss \citep{kingma2013auto}.

\subsubsection{Statistical Approaches}
\label{subsubsec:statsapproach}
Inspired by \citep{rubin1993statistical}, a variety of statistical approaches for synthetic data have been developed. Considering the 
broad range of these approaches, it is somehow difficult to provide a detailed overview which fully describes all types of synthetic data.
Nonetheless, despite the apparent distinction between those Computer Science based approaches, scientists have offered three alternative classification schemes, 
i.e.  sequential vs. joint modeling approaches, parametric vs. machine learning inspired approaches and extensions of these MI inspired approaches.


\paragraph{Sequential and Joint Modelling based Approaches}
Given any joint distribution can be rewritten as a product of conditional distributions, a majority of the early applications of data synthesis depended on a sequential modelling approach, where each 
variable is synthesized sequentially utilizing models conditional on other variables that were already synthesized or variables remaining 
unchanged. Great flexibility is offered by this sequential regression approach in that different models can be employed for each variable.
Based on this, we can utilize simple parametric models such as linear/logistic regression but also any machine learning algorithm allowing random
draws from a conditional distribution, e.g. Classification and Regression Trees \citep{loh2011classification}, short for CART, or Random Forest. Many efforts have been invested to develop a more portable and simple tool for
data synthsis, which results in the publishment of several packages. One exemplified case that worths mentioning is the development of the R package called \textit{synthpop} \citep{nowok2016synthpop}. Targeted to 
Though primarily targeted to mimicing data from some longitudinal studies and avoiding actual data disclosure, \textit{synthpop} is applicable to provide test synthetic data
for uses of confidentiality concerns. A variety of synthesizing methods are implemented in the package, including non-parametric models such as Conditional Inference Trees \citep{hothorn2015ctree} and CART, 
parametric models (e.g. Normal Linear Regression, Logistic Regression) and other methods such as ramdom sampling. Furthermore, \textit{synthpop} has also provided users with additional functionalities to deal with
missing data/restricted values involved context or limitation of perceived disclosure.

In comparison to the sequential modelling based approach, joint modelling is considered as an approach intended to directly specify the joint distribution of the data. In recent years, more flexible models have been developed
given several early approaches, such as the IPSO method \citep{burridge2003information}, relied on some assumption which is seldom validated with actual data. More specifically, an approach based on a Dirichlet Process 
Mixture of Products of Multinomials is proposed by \citet{hu2014disclosure}, which offers high utility in real data applications. Moreover, for continuous business data, the approach based on Dirichlet
Process Normal Mixture Models \citep{kim2018simultaneous} has proved to show good performance. To be more applicable for business surveys, the extension work of this approach \citep{kim2021synthetic} included the informative sampling designs.
\paragraph{Parametric and Machine Learning based Approaches}

\paragraph{Extensions of the MI inspired Approaches}


\subsubsection{Differentially Private Data Synthesis}
\label{subsubsec:dpds}



