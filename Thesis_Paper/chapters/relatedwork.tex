In this chapter, general definition of data privacy and the necessity to maintain data privacy are described. 
Furthermore, in order to keep data privacy, this chapter also presents an overview of methods to achieve the 
goal of privacy preserving data analysis and publication, which have been adopted in several domains. 
Here, we want to emphasize the use of synthetic data and one of its most popular applications, 
i.e., differentially private synthetic data, that are able to prevent disclosure in the process of synthetic data generation.

\subsection{Data Privacy}
\label{subsec:dataprivacy}
It is well-acknowledged that we have entered a data-driven world and data are 
often regarded as significant constituents for our society. At the same time, 
an open society can also learn from these data so as to develop feasible 
and practical policy guidelines \citep{evans2021statistically}. Especially 
during the outbreak of coronavirus disease 2019 (COVID-19), more and more increased concerns are raised that it is 
essential for a society to utilize such data, which are widely-spread in the population
and analyzed with regard to various perspectives, to advance sophiscated planning
and develop more concrete social welfare benefits for the citizens. Consequently,
both seen from the public health perspective and the economy perspective, the on-going
COVID-19 global pandemic serves as a rigid reminder that detailed data are urgently 
needed to assist in decision making, damage control scenarios. Regardless of prevalent
consensus reached to leverage more microdata, the inappropriate use of such 
information can cause harm in data confidentiality and privacy as sometimes the attacks
from an intruder may result in the leakage of an individual's sensitive information, e.g., identity, 
address and salary, etc.


On the premise of possible outcomes brought by the misuse of microdata, it is crucial that 
we encourage proper and legal use of the collected datasets. Holding this motivation, researchers 
have developed a variety of strategies aiming to avoid the disclosure of sensitive records while
revealing these specific information to the public \citep{duncan2011statistical}. In the early times, 
several traditional methods have been proposed to limit data disclosure with strategies like top-coding,
swapping or data suppression. Nevertheless, with increased computing power and more data access demanded by
the public, the risks of data disclosure are often seen as underestimated using simply these traditional
protection strategies, where instances of privacy breaches can be found simultaneously in the public and 
private sectors \citep{de2015unique}.

Alternatively, with the purpose to reach the trade-off between data disclosure protection and broad
data access, the idea of synthetic data has been released. When using this approach, we make a model
fitted to the microdata and the corresponding outputs from the fitted model are then used to replace
the original values in the previous information.


\subsection{An Overview of Data Synthesis Approaches}
\label{subsec:datasynthesis}
The field of employing synthetic data to avoid data statistical disclosure has been introduced by \citet{rubin1993statistical}
and \citet{little1993statistical} in the context of learning multiple imputation (MI) for nonresponse \citep{little2019statistical}.
In their work, they showed the possibility to get these sentitive values replaced with "imputed" values
rather than impute data for those missing records in the original dataset. With the application of this multiple
imputation framework, ramdom draws sampled from these imputed populations can then be circulated to the public. In more
extreme cases, when it is necessary to avoid the release of original data completely, those instances from
the original sample can also be displaced by samples from the imputation model.

Depending on the level of protection prone to specific scenarios, data synthesis methods are categorized to two groups, i.e.,
partial synthesis and full synthesis. For partially synthetic data, only some parts of the original records are synthesized. 
On the contrary, the entire dataset are replaced by synthetic values with the utilizatio of full synthesis methods specifically.
It is evident to infer that the desired protection level is high when applying fully synthetic data methods, as original instances
are completely excluded. 



\subsubsection{Computer Science Approaches}
\label{subsubsec:csapproach}
Despite the origin and early development of synthetic data, in computer science, the data Synthesis
approach did not raise much interest in the study of data privacy until the advent of various privacy
standards. In order to cater to people's requirement of privacy protection, scientists have defined
several popular data privacy standards such as $k-$anonymity \citep{sweeney2002k}, $l-$diversity \citep{machanavajjhala2007diversity}
and $t-$closeness \citep{li2006t}. We try to shortly introduce the key ideas of the three popular
standards in the following context. 

In 1998, \citet{sweeney2002k} has proposed the term of $k-$anonymity. When the information for every individual included in the data
cannot be discriminated from at least $k-1$ other individuals, the release of the database is considered to fulfill the
$k-$anonymity requirement. The works of \citet{machanavajjhala2007diversity}, \citet{truta2006privacy} and \citet{xiao2006personalized}
have exhibited that $k-$anonymity can successfully prevent identity disclosure and in this case it is impossible to identify
his exact data in the database. However, it does not guarantee attribute disclosure which indicates the revealing of a person's
sensitive attribute information even if we cannot access the exact record of instance from him/her. 

In order to overcome the limitations brought by $k-$anonymity, \citet{machanavajjhala2007diversity} introduced the definition of $l-$diversity in 2006 and the
essential idea lies in the prevention of homogeneity which often occurs in the equivalence classes with sensitive attributes.
By noting equivalence classes, we aim to refer those data instances that are unidentifiable from one another with masked information
in those non-sensitive attributes and thus can be categorized as an equivalence class. A database is considered to be $l-$diverse
if there are at least $l$ different values belonging to those sensitive attributes in each equivalence class. The shortcoming of $l-$ diverisity
is that this standard can be unnecessary to achieve from time to time. For instance, if we now have 10,000 rows of data with a sensitive attribute,
the distribution of the sensitive attribute is imbalanced where the attribute is 1 for $1\%$ of the individuals and 0 for the remaining
$99\%$. In an equivalence class with simply $0-$entries for the sensitive attribute, the individual would actually not mind being
recorded as negative for the sensitive attribute. But so as to achieve $2-$diversity, the number of equivalence classes would
then be $10,0000 \times 0.01 = 100$, which would cause a large amount of information loss.

The shortcomings of $l-$diverisity are addressed when \citet{xiao2006personalized} proposed the definition of $t-$closeness. We say that a database
is $t-$close when the distribution of the sensitive attributes in each equivalence class distinguishes from the distribution of the sensitive attributes 
in the complete table by a given threshold $t$ at most. The distance function used here is the Earth Mover Distance \citep{rubner2000earth} and It
calculates the minimal amount of endeavor required to transform one distribution to another by moving the mass distribution between those two.

However, the definition of synthetic data does not really fit into the requirement of the above examplified privacy standards. For example, a fully
synthetic dataset can still offer a high level of privacy protection but breaches the requirement of $k-$anonymity for any $k>1$.

The idea of Differential Privacy (DP) is introduced by \citet{dwork2008differential} in 2008, which has facilitated the application of
synthetic data in the computer science literature. In short, a database is said to be differential private when changing one
record in the database has a limited effect on the outputs of the running data mechanism. With a focus swittched to data mechanism, 
the concept of DP situates better with the key ideas of data synthesis.



\subsubsection{Statistical Approaches}
\label{subsubsec:statsapproach}

\subsubsection{Differentially Private Data Synthesis}
\label{subsubsec:dpds}



