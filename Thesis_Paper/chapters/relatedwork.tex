In this chapter, general definition of data privacy and the necessity to maintain data privacy are described. 
Furthermore, in order to keep data privacy, this chapter also presents an overview of methods to achieve the 
goal of privacy preserving data analysis and publication, which have been adopted in several domains. 
Here, we want to emphasize the use of synthetic data and one of its most popular applications, 
i.e., differentially private synthetic data, that are able to prevent disclosure in the process of synthetic data generation.

\subsection{Data Privacy}
\label{subsec:dataprivacy}
It is well-acknowledged that we have entered a data-driven world and data are 
often regarded as significant constituents for our society. At the same time, 
an open society can also learn from these data so as to develop feasible 
and practical policy guidelines \citep{evans2021statistically}. Especially 
during the outbreak of coronavirus disease 2019 (COVID-19), more and more increased concerns are raised that it is 
essential for a society to utilize such data, which are widely-spread in the population
and analyzed with regard to various perspectives, to advance sophiscated planning
and develop more concrete social welfare benefits for the citizens. Consequently,
both seen from the public health perspective and the economy perspective, the on-going
COVID-19 global pandemic serves as a rigid reminder that detailed data are urgently 
needed to assist in decision making, damage control scenarios. Regardless of prevalent
consensus reached to leverage more microdata, the inappropriate use of such 
information can cause harm in data confidentiality and privacy as sometimes the attacks
from an intruder may result in the leakage of an individual's sensitive information, e.g., identity, 
address and salary, etc.


On the premise of possible outcomes brought by the misuse of microdata, it is crucial that 
we encourage proper and legal use of the collected datasets. Holding this motivation, researchers 
have developed a variety of strategies aiming to avoid the disclosure of sensitive records while
revealing these specific information to the public \citep{duncan2011statistical}. In the early times, 
several traditional methods have been proposed to limit data disclosure with strategies like top-coding,
swapping or data suppression. Nevertheless, with increased computing power and more data access demanded by
the public, the risks of data disclosure are often seen as underestimated using simply these traditional
protection strategies, where instances of privacy breaches can be found simultaneously in the public and 
private sectors \citep{de2015unique}.

Alternatively, with the purpose to reach the trade-off between data disclosure protection and broad
data access, the idea of synthetic data has been released. When using this approach, we make a model
fitted to the microdata and the corresponding outputs from the fitted model are then used to replace
the original values in the previous information.


\subsection{An Overview of Data Synthesis Approaches}
\label{subsec:datasynthesis}
The field of employing synthetic data to avoid data statistical disclosure has been introduced by \citet{rubin1993statistical}
and \citet{little1993statistical} in the context of learning multiple imputation (MI) for nonresponse \citep{little2019statistical}.
In their work, they showed the possibility to get these sentitive values replaced with "imputed" values
rather than impute data for those missing records in the original dataset. With the application of this multiple
imputation framework, ramdom draws sampled from these imputed populations can then be circulated to the public. In more
extreme cases, when it is necessary to avoid the release of original data completely, those instances from
the original sample can also be displaced by samples from the imputation model.

Depending on the level of protection prone to specific scenarios, data synthesis methods are categorized to two groups, i.e.,
partial synthesis and full synthesis. For partially synthetic data, only some parts of the original records are synthesized. 
On the contrary, the entire dataset are replaced by synthetic values with the utilization of full synthesis methods.
It is evident to infer that the desired protection level is high when applying fully synthetic data methods, as original instances
are completely excluded. 



\subsubsection{Computer Science Approaches}
\label{subsubsec:csapproach}
Despite the origin and early development of synthetic data, in computer science, the data Synthesis
approach did not raise much interest in the study of data privacy until the advent of various privacy
standards. In order to cater to people's requirement of privacy protection, scientists have defined
several popular data privacy standards such as $k-$anonymity \citep{sweeney2002k}, $l-$diversity \citep{machanavajjhala2007diversity}
and $t-$closeness \citep{li2006t}. We try to shortly introduce the key ideas of the three popular
standards in the following context. 

\paragraph{$k-$Anonymity, $l-$Diversity and $t-$Closeness}
In 1998, \citet{sweeney2002k} has proposed the term of $k-$anonymity. When the information for every individual included in the data
cannot be discriminated from at least $k-1$ other individuals, the release of the database is considered to fulfill the
$k-$anonymity requirement. The works of \citet{machanavajjhala2007diversity}, \citet{truta2006privacy} and \citet{xiao2006personalized}
have exhibited that $k-$anonymity can successfully prevent identity disclosure and in this case it is impossible to identify
his exact data in the database. However, it does not guarantee attribute disclosure which indicates the revealing of a person's
sensitive attribute information even if we cannot access the exact record of instance from him/her. 

In order to overcome the limitations brought by $k-$anonymity, \citet{machanavajjhala2007diversity} introduced the definition of $l-$diversity in 2006 and the
essential idea lies in the prevention of homogeneity which often occurs in the equivalence classes with sensitive attributes.
By noting equivalence classes, we aim to refer those data instances that are unidentifiable from one another with masked information
in those non-sensitive attributes and thus can be categorized as an equivalence class. A database is considered to be $l-$diverse
if there are at least $l$ different values belonging to those sensitive attributes in each equivalence class. The shortcoming of $l-$ diverisity
is that this standard can be unnecessary to achieve from time to time. For instance, if we now have 10,000 rows of data with a sensitive attribute,
the distribution of the sensitive attribute is imbalanced where the attribute is 1 for $1\%$ of the individuals and 0 for the remaining
$99\%$. In an equivalence class with simply $0-$entries for the sensitive attribute, the individual would actually not mind being
recorded as negative for the sensitive attribute. But so as to achieve $2-$diversity, the number of equivalence classes would
then be $10,0000 \times 0.01 = 100$, which would cause a large amount of information loss.

The shortcomings of $l-$diverisity are addressed when \citet{xiao2006personalized} proposed the definition of $t-$closeness. We say that a database
is $t-$close when the distribution of the sensitive attributes in each equivalence class distinguishes from the distribution of the sensitive attributes 
in the complete table by a given threshold $t$ at most. The distance function used here is the Earth Mover Distance \citep{rubner2000earth} and it
calculates the minimal amount of endeavor required to transform one distribution to another by moving the mass distribution between those two.


However, the definition of synthetic data does not really fit into the requirement of the above examplified privacy standards. For example, a fully
synthetic dataset can still offer a high level of privacy protection but breaches the requirement of $k-$anonymity for any $k>1$.

\paragraph{Differential Privacy based Approaches}
The idea of Differential Privacy (DP) is introduced by \citet{dwork2008differential} in 2008, which has facilitated the application of
synthetic data in the computer science literature. In short, a database is said to be differential private when changing one
record in the database has a limited effect on the outputs of the running data mechanism. With a focus swittched to the data mechanism, 
the concept of DP situates better with the key ideas of data synthesis. In this case, finding a suitable synthesis mechanism which suffices
the requirements of Differential Privacy is the most essential part of the work. 


Based on the establishment of DP concept, \citet{barak2007privacy} has developed one of the first approaches leveraging a Fourier Transformation
and linear programming for those low-order contingency tables \citep{dwork2008differential}. There are also some other early applications, e.g., \citep{eno2008generating},
\citep{cano2010evaluation}, \citep{blum2013learning}, \citep{xiao2010differential}, etc. Besides, others have also established approaches adapting
ideas from the statistical community to the requirement of Differential Privacy. These works include \citep{abowd2008protective}, 
\citep{machanavajjhala2008privacy}, \citep{charest2011can} and \citep{mcclure2012differential}. Simultaneously, with the use of Bayesian networks, 
\citep{zhang2017privbayes} has accommodated the idea of synthetic data to synthesize multi-dimensional datasets, which is called PrivBayes. Moreover,
taking into account the dependency structure of the original data, Copula function has been utilized by \citet{li2014differentially} to combine the ideas from both techniques (DPCopula).

\paragraph{Generative Adversarial Networks based Approaches}
In parallel, proposed by \citet{goodfellow2020generative}, the idea of Generative Adversarial Networks (GANs) has caused an huge inspiration
in data synthesis research and practical applications in the corresponding computer science literature. Fundamentally, a GAN comprises two deep 
neural networks that are called a generator and a discriminator, respectively. The generator network basically generates fake data in order to
mislead the discriminator, whose task concerns the distinguishment of real data and fake data. In an iterative process, both networks
are improved. In this case, those final data generated by the generator network can be regarded as a variant of the synthetic data.
Early applications of GANs have focused on synthesizing image data, for example \citep{denton2015deep}, as GANs turned out to be extremely
successfully image detection, speech recognition and natural language processing. Nevertheless, researchers gradually turned their attention to
microdata synthesis (in the computer science literature, microdata can be understood as tabular data specifically in contrast to image data or other types of synthesis). 
But to adopt GANs for microdata synthsis requires additional concerns. Unlike those neighboring pixel data, microdata often consists of categorical variables
which can be sparse and correlations among them are often weaker. As a consequence, the positioning of variables with microdata is also
sparsely informative because of the independent structure of indivial instance records. Thus, the relationships between variables need to be 
additionally modelled without the help of spatial information. To address this issue and model relationships between variables, 
\citet*{xu2019modeling} developed the conditional tabular GAN (CTGAN) to overcome challenges from imbalanced categorical and multi-modal continuous data.
Furthermore, Causal Tabular GAN (Causal-TGAN) \citep{wen2021causal} shows the possibility to model for casual relationships between variables.

\paragraph{Other Techniques based Approaches}
Apart from the adoption of GANs, other data synthesis techniques relied on Variational Autoencoders \citep{vardhan2020generating,ma2020vaem},
copulas \citep{patki2016synthetic,kamthe2021copula}, or other strategies especially preserving certain marginal distributions \citep{mckenna2019graphical} have also
been developed recently. Here, we want to shortly cover the adaptation of Variational Autoencoders (VAEs) for synthetic data. Compared to GANs,
a typical VAE includes three networks: an encoder, a decoder and a discriminator, which are responsible for learning complimentary tasks. To be more specific,
the encoder maps the input data to a latent representation while the decoder tries to reconstruct and restore the data. For the discriminator, it decides
if the given sample is real data or data produced by the decoder network. The reconstruction error needs to be minimized during the training of VAE. 
One of its first applications is called the Triplet based Variational Autoencoder (TVAE), Proposed by \citet{xu2019modeling}, TVAE is adapted from VAE to tabular data, and
is trained using evidence lower-bound (ELBO) loss \citep{kingma2013auto}.

\subsubsection{Statistical Approaches}
\label{subsubsec:statsapproach}
Inspired by \citep{rubin1993statistical}, a variety of statistical approaches for synthetic data have been developed. Considering the 
broad range of these approaches, it is somehow difficult to provide a detailed overview which fully describes all types of synthetic data.
Nonetheless, despite the apparent distinction between those Computer Science based approaches, scientists have offered three alternative classification schemes, 
i.e.  sequential vs. joint modeling approaches, parametric vs. machine learning inspired approaches and extensions of these MI inspired approaches.


\paragraph{Sequential and Joint Modelling based Approaches}
Given any joint distribution can be rewritten as a product of conditional distributions, a majority of the early applications of data synthesis depended on a sequential modelling approach, where each 
variable is synthesized sequentially utilizing models conditional on other variables that were already synthesized or variables remaining 
unchanged. Great flexibility is offered by this sequential regression approach in that different models can be employed for each variable.
Based on this, we can utilize simple parametric models such as linear/logistic regression but also any machine learning algorithm allowing random
draws from a conditional distribution, e.g. Classification and Regression Trees \citep{loh2011classification}, short for CART, or Random Forest. Many efforts have been invested to develop a more portable and simple tool for
data synthsis, which results in the publishment of several packages. One exemplified case that worths mentioning is the development of the R package called \textit{synthpop} \citep{nowok2016synthpop}. Targeted to 
Though primarily targeted to mimicing data from some longitudinal studies and avoiding actual data disclosure, \textit{synthpop} is applicable to provide test synthetic data
for uses of confidentiality concerns. A variety of synthesizing methods are implemented in the package, including non-parametric models such as Conditional Inference Trees \citep{hothorn2015ctree} and CART, 
parametric models (e.g. Normal Linear Regression, Logistic Regression) and other methods such as ramdom sampling. Furthermore, \textit{synthpop} has also provided users with additional functionalities to deal with
missing data/restricted values involved context or limitation of perceived disclosure.

In comparison to the sequential modelling based approach, joint modelling is considered as an approach intended to directly specify the joint distribution of the data. In recent years, more flexible models have been developed
given several early approaches, such as the IPSO method \citep{burridge2003information}, relied on some assumption which is seldom validated with actual data. More specifically, an approach based on a Dirichlet Process 
Mixture of Products of Multinomials is proposed by \citet{hu2014disclosure}, which offers high utility in real data applications. Moreover, for continuous business data, the approach based on Dirichlet
Process Normal Mixture Models \citep{kim2018simultaneous} has proved to show good performance. To be more applicable for business surveys, the extension work of this approach \citep{kim2021synthetic} included the informative sampling designs.

\paragraph{Parametric and Machine Learning based Approaches}
To obtain valid inferences of the synthetic data, researchers tend to assume that the models used for data synthesis are correctly specifified, which means they need to match the actual data 
generating process. Additionally, the synthesis model is required to be congenial to the analysis model later run on the synthetic data. To be more specific, congeniality \citep{meng1994multiple} indicates that the synthesis model
relies on the same modelling assumptions as the model used for analysis.

In practice, achieving congeniality is a pessimistic purpose, because it is not possible to foresee all the analyses to be run on the synthetic data. Nevertheless, shown in the nonresponse context \citep{meng1994multiple},
approximately valid inferences can be acquired when the synthesis model embraces the analysis model, meaning the former includes more variables than the latter. Given this, it is normally advised to utilize a rich set of predictors in the synthesis model,
which are indeally conditional on all the other variables and also includes interaction terms \citep{little1997should}. Still, the strategy does not really fit in parametric models, as it is often the case that many datasets include plenties of variables.
For categorical variables especially, there exist collinearity issues and perfect prediction usually comes with many variables used to fit the parametric model, which is not feasible any more. 

To address the uncongeniality concern, researches turn to Machine Learning algorithms for inspirations finding alternative synthesis strategies. For instance, \citet{reiter2005using} proposed using CART for data synthesis and later \citet{caiola2010random} extended this context to
Random Forests. It is important to note that these Machine Learning based approaches tend to automatically recognize higher order interactions which are often overlooked in parametric models.
In addition, multicollinearity or perfect prediction are no longer a cause of problem, and can still be directly applied when the number of variables exceeds the number of observations.

\paragraph{Extensions of the MI inspired Approaches}
Despite classical synthesis approaches, various extensions have also been explored and the differences mainly lie in the corresponding inferential procedures. In 2004,  
\citet{reiter2004simultaneous} proposed the first extension of the classical MI based synthesizing approach, which provides a strategy for missing data and disclosure control. Specifically,
a two-step procesure was developed, where missing data are imputed $m$ times at the first phase and $r$ partially synthetic datasets are generated at the second phase within the nest of each
first phase.

Likewise, \citet{reiter2010releasing} developed a two-stage synthesizing approach, where variables having a higher disclosure risk are synthesized at the first stage and variables requiring dozens of synthetic datasets to reduce model uncertainty
will be synthesized at the second stage. The authors have managed to show that this approach offers better disclosure limitation and similar data utility in comparison to classical one-stage synthesis using the same number of synthetic datasets.

The final methodology was presented by \citet{drechsler2010sampling}, which requires to use a (sub)sampling step before the synthesis. This approach is specifically suitable for Census data in that
it is common practice to release only random samples from the full dataset to the public. Furthermore, the synthesis models can be estimated using the full dataset even though just a (sub)sample is synthesized later.
With a real dataset, the authors have illustrated that the release of synthetic samples can offer higher utility than of the original data, as the synthesis models rely on the information from the full population. This methodology was later extended to the 
context in which the original data itself is already a sample \citep{drechsler2012combining}.

\subsubsection{Differentially Private Data Synthesis}
\label{subsubsec:dpds}
Prevalently adopted as a definition of privacy offering formal in recent years, differential privacy \citep{dwork2006calibrating}, abbreviated as DP, has offered mathematically quantifiable privacy guarantees to privacy related scenarios.
According to DP, it is required the impact that any single record can have on the probability of acquiring a specific result is bounded strictly. To be more specific, the definition of pure$\epsilon-$DP requires that the log$-$difference in the probability
of acquiring a specific output between two neighboring datasets is bounded between $-\epsilon$ and $\epsilon$. The neighboring datasets can be understood as datasets that differ in just one record. In simple termns, an algorithm is said to be differentially
private when someone obtaining the output statistic cannot distinguish whether the information of an individual was utilized in the computation or not.

Simultaneously, the definition of DP has also boosted research focusing on generating differentially private synthetic data. Compared to other synthesizing approaches, differentially private synthetic data outstands the others in that any function of a differentially
private output is guaranteed to also be differentially private with the same privacy guarantees as the original output. In other words, working with the differentially private synthetic data is more flexible, where it allows more interaction with the data and using any
tools or workflows to process the data, without suffering the risk to unveiling sensitive information.

Researchers have proposed various ideas to generate differentially private synthetic data, where using marginal distributions for data synthesis has been one of the most prevalent approaches. Particularly, noise is added to either one$-$, two$-$, or three$-$way marginal distributions
as specified by \citet{mckenna2019graphical}. Another inspiration comes from the adoption of Bayesian Networks \citep{bao2021synthetic}, within which the use of PrivBayes \citet{zhang2017privbayes} are more prominent. However, it can be difficult to 
represent all important correlations in PrivBayes. To overcome this, \citet{cai2021data} utilized a defined Markov random field (MRF) to model the correlations among variables and later the MRF for synthesizing data (PrivMRF).


